================================================================================
                    PVP AI TRAINING GUIDE - COMPLETE WALKTHROUGH
================================================================================

TABLE OF CONTENTS:
1. System Architecture Overview
2. Port Configuration
3. Training Workflow Step-by-Step
4. Python training_loop.py Setup
5. Reward System Design
6. Event Handling
7. Multi-Agent Training
8. Troubleshooting

================================================================================
1. SYSTEM ARCHITECTURE OVERVIEW
================================================================================

The system consists of 3 main components:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Minecraft Mod  â”‚â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  IPC (Sockets)   â”‚â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  Python AI      â”‚
â”‚  (Client-Side)  â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”‚  Frame + Events  â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”‚  training_loop  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚                           â”‚                            â”‚
      â”‚ Captures:                 â”‚ Sends:                     â”‚ Processes:
      â”‚ - Screen frames           â”‚ - RGB frames               â”‚ - Observations
      â”‚ - Player state            â”‚ - Game state               â”‚ - Actions
      â”‚ - Hit/Death events        â”‚ - Events                   â”‚ - Rewards
      â”‚                           â”‚ - Commands                 â”‚ - Training


HOW IT WORKS:
-------------
1. Minecraft client captures game state every frame (60 FPS)
2. Mod sends frame data + events to Python via TCP sockets
3. Python AI receives data, calculates reward, decides action
4. Python sends action back to mod (move, attack, etc.)
5. Mod executes action in-game
6. Repeat â†’ AI learns over time!


================================================================================
2. PORT CONFIGURATION
================================================================================

PORT LAYOUT:
------------
Port 9998:  Command Channel (START/STOP/RESET commands)
Port 9999:  Agent 1 - Frame data stream
Port 10001: Agent 2 - Frame data stream
Port 10002: Agent 3 - Frame data stream
Port 10003: Agent 4 - Frame data stream
... (and so on for more agents)


IMPORTANT:
----------
- Each Minecraft client connects to ONE port (one agent)
- Python training_loop.py must listen on ALL agent ports simultaneously
- Command port (9998) is shared across all agents


================================================================================
3. TRAINING WORKFLOW STEP-BY-STEP
================================================================================

STEP 1: START PYTHON TRAINING LOOP
-----------------------------------
1. Open terminal
2. Navigate to Python project folder
3. Run: python training_loop.py
4. Python should print:
   - "Listening on port 9999 for Agent 1"
   - "Listening on port 10001 for Agent 2"
   - "Command server listening on port 9998"


STEP 2: START MINECRAFT CLIENT(S)
----------------------------------
1. Launch Minecraft with the mod installed
2. Join a server (or singleplayer)
3. Mod automatically connects to port 9999 (Agent 1)
4. Python should print: "Agent 1 connected!"


STEP 3: START TRAINING
-----------------------
In Minecraft, type one of these:

CLIENT-SIDE (works without server mod):
  /ki start    â†’ Sends START command to Python
  /ki stop     â†’ Sends STOP command to Python
  /ki reset    â†’ Resets rewards to zero (no teleport)

SERVER-SIDE (requires mod on server):
  /ki start    â†’ Server sends START to Python on port 9998
  /ki reset <p1> <p2> <kit> â†’ Teleports players and applies kit


STEP 4: MONITOR TRAINING
-------------------------
- Python should print events as they happen:
  * "Agent 1: EVENT:HIT:Player1:Player2"
  * "Agent 1: EVENT:DEATH:Player2:Player1"
  * "Agent 1: Reward: +10.5"
  
- In Minecraft, use:
  /testframe  â†’ Check IPC connection status
  /name       â†’ Toggle nametag overlays


STEP 5: MULTI-AGENT TRAINING (Optional)
----------------------------------------
For training 2 agents against each other:

1. Start Python with 2 agent listeners (ports 9999 and 10001)
2. Launch first Minecraft client â†’ connects to Agent 1 (port 9999)
3. In first client, type: /agent 2
4. Launch second Minecraft client â†’ automatically connects to Agent 2 (10001)
5. Both agents now train simultaneously!


================================================================================
4. PYTHON training_loop.py SETUP
================================================================================

YOUR training_loop.py SHOULD:
------------------------------

A) LISTEN ON MULTIPLE PORTS:
```python
import socket
import threading
import json
import struct

# Configuration
COMMAND_PORT = 9998
AGENT_PORTS = [9999, 10001, 10002]  # Support 3 agents

# Global state
agents = {}  # {agent_id: {socket, state, model, etc}}
training_active = False

def handle_command_port():
    """Listen for START/STOP/RESET commands"""
    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    sock.bind(('127.0.0.1', COMMAND_PORT))
    sock.listen(1)
    
    while True:
        conn, addr = sock.accept()
        # Read command (4 bytes length + JSON)
        length = struct.unpack('>I', conn.recv(4))[0]
        data = json.loads(conn.recv(length).decode('utf-8'))
        
        cmd_type = data.get('type')
        if cmd_type == 'START':
            training_active = True
            print("Training STARTED")
        elif cmd_type == 'STOP':
            training_active = False
            print("Training STOPPED")
        elif cmd_type == 'RESET':
            # Reset rewards for all agents
            for agent in agents.values():
                agent['cumulative_reward'] = 0
            print("Rewards RESET")

def handle_agent(agent_id, port):
    """Handle one agent's frame stream"""
    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    sock.bind(('127.0.0.1', port))
    sock.listen(1)
    print(f"Agent {agent_id} listening on port {port}")
    
    while True:
        conn, addr = sock.accept()
        print(f"Agent {agent_id} connected!")
        
        while True:
            try:
                # Read header length (4 bytes)
                header_len = struct.unpack('>I', conn.recv(4))[0]
                if header_len == 0:
                    break
                
                # Read header (JSON)
                header_data = conn.recv(header_len).decode('utf-8')
                header = json.loads(header_data)
                
                # Read body (frame image)
                body_len = header.get('bodyLength', 0)
                frame_bytes = conn.recv(body_len)
                
                # Process frame
                process_frame(agent_id, header, frame_bytes, conn)
                
            except Exception as e:
                print(f"Agent {agent_id} disconnected: {e}")
                break

def process_frame(agent_id, header, frame_bytes, conn):
    """Process one frame from an agent"""
    global training_active
    
    # Extract data from header
    events = header.get('events', [])
    player_name = header.get('player_name', 'Unknown')
    health = header.get('health', 20)
    position = header.get('position', {})
    
    # Handle commands (START/STOP/RESET sent via IPC)
    if 'cmd_type' in header:
        cmd_type = header['cmd_type']
        if cmd_type == 'START':
            training_active = True
            print(f"Agent {agent_id}: Training STARTED")
        elif cmd_type == 'STOP':
            training_active = False
            print(f"Agent {agent_id}: Training STOPPED")
        elif cmd_type == 'RESET':
            agents[agent_id]['cumulative_reward'] = 0
            print(f"Agent {agent_id}: Rewards RESET")
    
    if not training_active:
        return
    
    # Calculate reward based on events
    reward = 0.0
    for event in events:
        if event.startswith('EVENT:HIT:'):
            parts = event.split(':')
            attacker = parts[2]
            target = parts[3]
            
            if attacker == player_name:
                reward += 1.0  # Agent hit someone
                print(f"Agent {agent_id}: Hit {target} (+1.0)")
            elif target == player_name:
                reward -= 0.5  # Agent got hit
                print(f"Agent {agent_id}: Got hit by {attacker} (-0.5)")
        
        elif event.startswith('EVENT:DEATH:'):
            parts = event.split(':')
            victim = parts[2]
            killer = parts[3]
            
            if killer == player_name:
                reward += 10.0  # Agent killed someone!
                print(f"Agent {agent_id}: Killed {victim} (+10.0)")
            elif victim == player_name:
                reward -= 5.0  # Agent died
                print(f"Agent {agent_id}: Died to {killer} (-5.0)")
    
    # Convert frame to image
    # frame_bytes contains raw RGB data
    # You need to reshape it based on resolution
    # Example: 640x360 resolution = 640*360*3 bytes
    
    # TODO: Feed frame to neural network
    # action = model.predict(frame)
    
    # Send action back to Minecraft
    action = {
        'forward': 0,    # 0 or 1
        'backward': 0,
        'left': 0,
        'right': 0,
        'jump': 0,
        'sneak': 0,
        'attack': 0,
        'use': 0,
        'yaw': 0.0,      # Mouse movement
        'pitch': 0.0
    }
    
    # Send action (2 bytes length + JSON)
    action_json = json.dumps(action).encode('utf-8')
    conn.sendall(struct.pack('>H', len(action_json)))
    conn.sendall(action_json)
    
    # Update agent stats
    agents[agent_id]['cumulative_reward'] += reward

# Start threads
threading.Thread(target=handle_command_port, daemon=True).start()

for i, port in enumerate(AGENT_PORTS):
    agent_id = i + 1
    agents[agent_id] = {'cumulative_reward': 0}
    threading.Thread(target=handle_agent, args=(agent_id, port), daemon=True).start()

# Keep main thread alive
while True:
    time.sleep(1)
```


================================================================================
5. REWARD SYSTEM DESIGN
================================================================================

RECOMMENDED REWARD STRUCTURE:
-----------------------------

POSITIVE REWARDS (Agent does well):
  +1.0   â†’ Hit an enemy
  +10.0  â†’ Kill an enemy
  +0.1   â†’ Per second alive
  +5.0   â†’ Win a round

NEGATIVE REWARDS (Agent does poorly):
  -0.5   â†’ Got hit by enemy
  -5.0   â†’ Death
  -10.0  â†’ Death to environment (fall damage, etc.)
  -0.01  â†’ Per frame (encourage efficiency)

OPTIONAL SHAPING REWARDS:
  +0.1   â†’ Moving towards enemy
  +0.5   â†’ Looking at enemy
  -0.1   â†’ Standing still too long
  +2.0   â†’ Dodge/block enemy attack


CONFIGURABLE REWARDS:
---------------------
Create a config.json file:

{
  "reward_hit_dealt": 1.0,
  "reward_kill": 10.0,
  "penalty_hit_taken": -0.5,
  "penalty_death": -5.0,
  "penalty_death_environment": -10.0,
  "reward_alive_per_second": 0.1
}

Load in Python:
```python
with open('config.json') as f:
    config = json.load(f)

reward_hit = config['reward_hit_dealt']
```


================================================================================
6. EVENT HANDLING
================================================================================

EVENTS SENT FROM MOD TO PYTHON:
--------------------------------

1. EVENT:HIT:attacker:target
   - Triggered when attacker hits target
   - Works on any server (client-side detection)
   - Example: "EVENT:HIT:Player1:Player2"

2. EVENT:DEATH:victim:killer
   - Triggered when victim dies
   - killer = player name OR "Environment"
   - Works on any server (chat message parsing)
   - Example: "EVENT:DEATH:Player2:Player1"
   - Example: "EVENT:DEATH:Player1:Environment"


FRAME HEADER FORMAT:
--------------------
{
  "events": ["EVENT:HIT:...", "EVENT:DEATH:..."],
  "player_name": "YourPlayerName",
  "agent_id": 1,
  "health": 20.0,
  "position": {"x": 100, "y": 64, "z": 200},
  "yaw": 90.0,
  "pitch": 0.0,
  "bodyLength": 691200,  // Frame size in bytes
  "cmd_type": "START",   // Optional command
  "cmd_data": "..."      // Optional command data
}


================================================================================
7. MULTI-AGENT TRAINING
================================================================================

TRAINING 2 AGENTS AGAINST EACH OTHER:
--------------------------------------

SETUP:
1. Python listens on ports 9999 and 10001
2. Launch Minecraft client 1 â†’ Agent 1 (auto-connects to 9999)
3. Launch Minecraft client 2 â†’ Agent 2 (auto-connects to 10001)
4. Both agents train simultaneously!

COMMANDS:
- Agent 1: Already on port 9999
- Agent 2: Type /agent 2 to switch to port 10001

BENEFITS:
- Self-play training (agent learns from itself)
- Faster training (parallel data collection)
- More realistic combat scenarios


SCALING TO MORE AGENTS:
-----------------------
1. Add more ports to AGENT_PORTS list in Python
2. Launch more Minecraft clients
3. Use /agent <id> to assign each client to an agent


================================================================================
8. TROUBLESHOOTING
================================================================================

PROBLEM: "IPC not active - Python not connected"
SOLUTION:
  - Make sure training_loop.py is running FIRST
  - Check Python is listening on the correct port
  - Verify firewall isn't blocking connections
  - Use /testframe command to check status

PROBLEM: "Frames not being sent"
SOLUTION:
  - Check console output for "[IPC Port 9999] Frame sent"
  - Verify Python is reading frames correctly
  - Check socket connection isn't timing out

PROBLEM: "Events not detected"
SOLUTION:
  - Hit events: Check AttackEntityCallback is registered
  - Death events: Check chat messages are being parsed
  - Use /testframe to verify IPC is active

PROBLEM: "Port 10001 gets skipped"
SOLUTION:
  - Already fixed! Command port moved to 9998
  - Agent 1 = 9999, Agent 2 = 10001, Agent 3 = 10002

PROBLEM: "Nametags not rendering"
SOLUTION:
  - Create team: /team create myteam
  - Add player: /team add myteam PlayerName
  - Teams are now synced to clients automatically!
  - Toggle nametags: /name

PROBLEM: "Rewards not tracking"
SOLUTION:
  - Make sure /ki start was called
  - Check Python is processing events correctly
  - Verify event format matches expected pattern
  - Check cumulative_reward is being updated


================================================================================
QUICK REFERENCE - COMMANDS
================================================================================

CLIENT-SIDE COMMANDS (work on any server):
  /ki start          â†’ Start reward tracking
  /ki stop           â†’ Stop reward tracking
  /ki reset          â†’ Reset rewards (no teleport)
  /testframe         â†’ Check IPC connection status
  /name              â†’ Toggle nametag overlays
  /agent <id>        â†’ Switch to agent ID (changes port)
  /kit create <name> â†’ Save current inventory as kit
  /kit load <name>   â†’ Load kit
  /kit list          â†’ List all kits
  /kit sync          â†’ Sync client kits to server

SERVER-SIDE COMMANDS (require mod on server):
  /ki start          â†’ Start tracking (sends to port 9998)
  /ki stop           â†’ Stop tracking
  /ki reset <p1> <p2> <kit> [shuffle] â†’ Reset players with kit
  /team create <name>      â†’ Create team
  /team add <team> <player> â†’ Add player to team
  /team remove team <name>  â†’ Remove team
  /team remove player <name> â†’ Remove player from team
  /team list         â†’ List all teams
  /team clear        â†’ Clear all teams


================================================================================
FINAL NOTES
================================================================================

1. The mod is ONLY a data bridge - it sends observations and receives actions
2. ALL AI logic, neural networks, training happens in Python
3. Reward values are 100% customizable in training_loop.py
4. Events work on ANY Minecraft server (vanilla, modded, etc.)
5. Multi-agent training requires multiple Minecraft clients
6. Start Python BEFORE launching Minecraft!

Good luck with your training! ğŸš€
================================================================================
