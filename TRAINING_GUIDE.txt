================================================================================
                    PVP AI TRAINING GUIDE - COMPLETE WALKTHROUGH
================================================================================

‚ö†Ô∏è IMPORTANT: This project already has a complete training system implemented!
   You don't need to write Python code - just use the provided files.

TABLE OF CONTENTS:
1. System Architecture Overview
2. Port Configuration  
3. Training Workflow Step-by-Step
4. Understanding the Python Code (Already Implemented!)
5. Reward System Configuration (GUI-Based)
6. Event Handling
7. Multi-Agent Training
8. Troubleshooting

================================================================================
1. SYSTEM ARCHITECTURE OVERVIEW
================================================================================

The system consists of 3 main components:

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Minecraft Mod  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  IPC (Sockets)   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Python AI      ‚îÇ
‚îÇ  (Client-Side)  ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  Frame + Events  ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  training_loop  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      ‚îÇ                           ‚îÇ                            ‚îÇ
      ‚îÇ Captures:                 ‚îÇ Sends:                     ‚îÇ Processes:
      ‚îÇ - Screen frames           ‚îÇ - RGB frames               ‚îÇ - Observations
      ‚îÇ - Player state            ‚îÇ - Game state               ‚îÇ - Actions
      ‚îÇ - Hit/Death events        ‚îÇ - Events                   ‚îÇ - Rewards
      ‚îÇ                           ‚îÇ - Commands                 ‚îÇ - Training


HOW IT WORKS:
-------------
1. Minecraft client captures game state every frame (60 FPS)
2. Mod sends frame data + events to Python via TCP sockets
3. Python AI receives data, calculates reward, decides action
4. Python sends action back to mod (move, attack, etc.)
5. Mod executes action in-game
6. Repeat ‚Üí AI learns over time!


================================================================================
2. PORT CONFIGURATION
================================================================================

PORT LAYOUT:
------------
Port 9998:  Command Channel (START/STOP/RESET commands)
Port 9999:  Agent 1 - Frame data stream
Port 10001: Agent 2 - Frame data stream
Port 10002: Agent 3 - Frame data stream
Port 10003: Agent 4 - Frame data stream
... (and so on for more agents)


IMPORTANT:
----------
- Each Minecraft client connects to ONE port (one agent)
- Python training_loop.py must listen on ALL agent ports simultaneously
- Command port (9998) is shared across all agents


================================================================================
3. TRAINING WORKFLOW STEP-BY-STEP
================================================================================

STEP 1: START PYTHON GUI
-------------------------
1. Open terminal in the "PVP KI" folder
2. Make sure you have dependencies installed:
   pip install torch numpy tkinter
3. Run: python training_loop.py
4. A GUI window will open titled "Multi-Agent PVP Training"
5. You'll see 2 agent panels by default (Agent 1 and Agent 2)


STEP 2: CONFIGURE REWARDS (IN THE GUI)
---------------------------------------
Each agent panel has reward sliders:
  - Win: +500.0 (default)
  - Loss: -500.0 (default)
  - Dmg Dealt: +10.0 (default)
  - Dmg Taken: -10.0 (default)
  - Time: -0.1 (default - penalty per frame)
  - Team Hit: -50.0 (default)
  - Team Kill: -500.0 (default)

You can adjust these values BEFORE starting training!
Use "Apply to All" button to copy settings to all agents.


STEP 3: START MINECRAFT CLIENT(S)
----------------------------------
1. Launch Minecraft with the mod installed
2. Join a server (or singleplayer)
3. Mod automatically connects to port 9999 (Agent 1)


STEP 4: START TRAINING (CLICK IN GUI)
--------------------------------------
1. In the GUI, click the "Start" button for Agent 1
2. The agent panel will show "Status: Running"
3. When Minecraft connects, you'll see "Connected!" in the agent log
4. Training starts automatically - no /ki start command needed!


STEP 5: MONITOR TRAINING (IN THE GUI)
--------------------------------------
Each agent panel shows:
  - Status: Running/Stopped/Stopping
  - Current Reward (resets on death)
  - Event log (shows hits, deaths, wins, losses)
  
Bottom panel shows:
  - Total fights completed
  - PPO updates performed  
  - Average policy/value loss
  - Entropy (exploration metric)

In Minecraft:
  /testframe  ‚Üí Check IPC connection status
  /name       ‚Üí Toggle nametag overlays


STEP 6: MULTI-AGENT TRAINING (2v2 or more)
-------------------------------------------
For self-play training with 2 agents:

1. In GUI, both Agent 1 and Agent 2 panels are already created
2. Click "Start" on both agents
3. Launch first Minecraft client ‚Üí Auto-connects to Agent 1 (port 9999)
4. Launch second Minecraft client ‚Üí Auto-connects to Agent 2 (port 10001)
5. Both agents train against each other!

To add more agents:
  - Click "+ Add Agent" in the GUI
  - Launch more Minecraft clients
  - Each client auto-connects to the next available port


STEP 7: SAVING THE MODEL
-------------------------
The model auto-saves after every fight ends (on death)!
Manual save: Click "Save Model" button in GUI

Checkpoints are saved to: checkpoints/pvp_model_YYYYMMDD_HHMMSS.pth


================================================================================
4. UNDERSTANDING THE PYTHON CODE (Already Implemented!)
================================================================================

YOU DON'T NEED TO WRITE CODE! The system is already complete.

FILES PROVIDED:
---------------
1. training_loop.py    - Main GUI and agent coordination
2. model.py            - Neural network (CNN + Actor-Critic)
3. ppo_trainer.py      - PPO algorithm implementation
4. ipc_client.py       - Socket communication utilities
5. drl_agent.py        - Additional agent utilities

HOW IT WORKS:
-------------
A) training_loop.py creates a GUI with:
   - Agent panels for each Minecraft client
   - Reward configuration sliders
   - Real-time status and metrics display
   - Start/Stop controls for each agent

B) Each agent connects to Minecraft via TCP socket:
   - Minecraft MOD ‚Üí connects to ‚Üí Python on ports 9999, 10001, etc.
   - Python receives: Frame data + Events + Game state
   - Python sends: Actions (movement, attack, look direction)

C) Neural Network (model.py):
   - Input: 64x64 RGB game frames
   - CNN extracts visual features
   - Actor outputs: Movement (W/A/S/D/Space/Attack) + Look (Yaw/Pitch)
   - Critic outputs: State value (for PPO training)

D) PPO Training (ppo_trainer.py):
   - Collects experiences in a shared buffer
   - Computes advantages using GAE (Generalized Advantage Estimation)
   - Updates policy with clipped objective
   - Auto-saves checkpoints after each fight

E) Reward Calculation:
   - Configured via GUI sliders (see Step 2 above)
   - Applied automatically based on events from Minecraft
   - Example: Hit enemy ‚Üí +10.0, Got hit ‚Üí -10.0, Win ‚Üí +500.0

THE SYSTEM IS FULLY AUTOMATED!
-------------------------------
Once you:
1. Start the GUI (python training_loop.py)
2. Click "Start" on an agent
3. Launch Minecraft

Everything else happens automatically:
  ‚úÖ Frame capture and sending
  ‚úÖ Event detection (hits, deaths)
  ‚úÖ Reward calculation
  ‚úÖ Neural network inference
  ‚úÖ Action execution in-game
  ‚úÖ PPO training updates
  ‚úÖ Model checkpointing


================================================================================
5. REWARD SYSTEM CONFIGURATION (GUI-Based - No Code Needed!)
================================================================================

REWARD CONFIGURATION IN THE GUI:
---------------------------------
Each agent panel has 7 reward sliders you can adjust IN REAL-TIME:

1. WIN REWARD (+500.0 default)
   - Given when the agent wins a fight (enemy dies, agent survives)
   - Higher = More motivation to win
   - Resets agent's cumulative reward to 0 after win

2. LOSS PENALTY (-500.0 default)  
   - Given when the agent loses (agent dies)
   - More negative = More punishment for dying
   - Resets agent's cumulative reward to 0 after loss

3. DAMAGE DEALT (+10.0 default)
   - Given for each hit the agent lands on enemy
   - Detected via EVENT:HIT events from mod
   - Encourages aggressive play

4. DAMAGE TAKEN (-10.0 default)
   - Given when agent loses health
   - Calculated from health delta in frame header
   - Encourages defensive play and dodging

5. TIME PENALTY (-0.1 default)
   - Applied EVERY FRAME
   - Encourages fast, decisive combat
   - Prevents passive/camping strategies
   - Total penalty = -0.1 √ó frames_per_fight

6. TEAM HIT PENALTY (-50.0 default)
   - Given when agent hits a teammate
   - Only applies in team modes
   - Discourages friendly fire

7. TEAM KILL PENALTY (-500.0 default)
   - Given when agent kills a teammate
   - Severe punishment for team kills
   - Only applies in team modes


HOW TO ADJUST REWARDS:
-----------------------
1. Type new values in the text boxes next to each reward
2. Press Enter to apply
3. Changes take effect IMMEDIATELY (even during active training!)
4. Use "Apply to All" button to copy settings to all agents


RECOMMENDED CONFIGURATIONS:
---------------------------

AGGRESSIVE FIGHTER:
  Win: +500, Loss: -300, Dmg Dealt: +20, Dmg Taken: -5, Time: -0.2

DEFENSIVE FIGHTER:
  Win: +800, Loss: -100, Dmg Dealt: +5, Dmg Taken: -30, Time: -0.05

BALANCED:
  Win: +500, Loss: -500, Dmg Dealt: +10, Dmg Taken: -10, Time: -0.1

SPEEDRUNNER (Fast kills):
  Win: +1000, Loss: -200, Dmg Dealt: +15, Dmg Taken: -5, Time: -1.0


HOW REWARDS ARE CALCULATED:
----------------------------
The training_loop.py automatically:

1. Monitors health changes ‚Üí applies Damage Taken penalty
2. Detects EVENT:HIT events ‚Üí applies Damage Dealt reward
3. Detects EVENT:DEATH events ‚Üí applies Win/Loss rewards
4. Applies Time Penalty every frame
5. Accumulates total reward and displays in GUI
6. Resets to 0 after each fight ends


================================================================================
6. EVENT HANDLING
================================================================================

EVENTS SENT FROM MOD TO PYTHON:
--------------------------------

1. EVENT:HIT:attacker:target
   - Triggered when attacker hits target
   - Works on any server (client-side detection)
   - Example: "EVENT:HIT:Player1:Player2"

2. EVENT:DEATH:victim:killer
   - Triggered when victim dies
   - killer = player name OR "Environment"
   - Works on any server (chat message parsing)
   - Example: "EVENT:DEATH:Player2:Player1"
   - Example: "EVENT:DEATH:Player1:Environment"


FRAME HEADER FORMAT:
--------------------
{
  "events": ["EVENT:HIT:...", "EVENT:DEATH:..."],
  "player_name": "YourPlayerName",
  "agent_id": 1,
  "health": 20.0,
  "position": {"x": 100, "y": 64, "z": 200},
  "yaw": 90.0,
  "pitch": 0.0,
  "bodyLength": 691200,  // Frame size in bytes
  "cmd_type": "START",   // Optional command
  "cmd_data": "..."      // Optional command data
}


================================================================================
7. MULTI-AGENT TRAINING (Self-Play)
================================================================================

WHAT IS SELF-PLAY?
------------------
Training multiple copies of the same agent against each other.
All agents share the SAME neural network weights.
This creates a "curriculum" where agents continuously adapt to their own strategies.

BENEFITS:
- Agents never plateau - always facing equally skilled opponents
- Discovers creative strategies and counter-strategies
- Much faster than training against fixed opponents
- No need for human players!


SETUP FOR 2-AGENT SELF-PLAY:
-----------------------------
1. Start training_loop.py GUI
2. You'll see Agent 1 and Agent 2 panels (already created by default)
3. Click "Start" on BOTH agents
4. Launch Minecraft client 1 ‚Üí Auto-connects to port 9999 (Agent 1)
5. Launch Minecraft client 2 ‚Üí Auto-connects to port 10001 (Agent 2)
6. Put them in the same Minecraft world
7. Let them fight!

IMPORTANT:
- Both agents share the same model (controlled by shared_model flag)
- Each fight improves BOTH agents simultaneously
- Model auto-saves after every fight ends


ADDING MORE AGENTS (3+ Players):
---------------------------------
1. In GUI, click "+ Add Agent" button
2. Agent 3 appears on port 10002
3. Launch Minecraft client 3 ‚Üí Auto-connects to 10002
4. Repeat for Agent 4, 5, etc.

TEAM VS TEAM (2v2, 3v3):
------------------------
1. Create 4 agents in GUI
2. Launch 4 Minecraft clients
3. In Minecraft, use team commands:
   /team create red
   /team create blue
   /team add red Agent1Name
   /team add red Agent2Name
   /team add blue Agent3Name
   /team add blue Agent4Name
4. Agents will avoid hitting teammates (Team Hit/Kill penalties apply)


PORT ALLOCATION:
----------------
Agent 1: Port 9999
Agent 2: Port 10001 (skips 10000)
Agent 3: Port 10002
Agent 4: Port 10003
... and so on

Each Minecraft client auto-connects to the next available port.


================================================================================
8. TROUBLESHOOTING
================================================================================

PROBLEM: "IPC not active - Python not connected"
SOLUTION:
  - Make sure training_loop.py is running FIRST
  - Check Python is listening on the correct port
  - Verify firewall isn't blocking connections
  - Use /testframe command to check status

PROBLEM: "Frames not being sent"
SOLUTION:
  - Check console output for "[IPC Port 9999] Frame sent"
  - Verify Python is reading frames correctly
  - Check socket connection isn't timing out

PROBLEM: "Events not detected"
SOLUTION:
  - Hit events: Check AttackEntityCallback is registered
  - Death events: Check chat messages are being parsed
  - Use /testframe to verify IPC is active

PROBLEM: "Port 10001 gets skipped"
SOLUTION:
  - Already fixed! Command port moved to 9998
  - Agent 1 = 9999, Agent 2 = 10001, Agent 3 = 10002

PROBLEM: "Nametags not rendering"
SOLUTION:
  - Create team: /team create myteam
  - Add player: /team add myteam PlayerName
  - Teams are now synced to clients automatically!
  - Toggle nametags: /name

PROBLEM: "Rewards not tracking"
SOLUTION:
  - Make sure /ki start was called
  - Check Python is processing events correctly
  - Verify event format matches expected pattern
  - Check cumulative_reward is being updated


================================================================================
QUICK REFERENCE - COMMANDS
================================================================================

CLIENT-SIDE COMMANDS (work on any server):
  /ki start          ‚Üí Start reward tracking
  /ki stop           ‚Üí Stop reward tracking
  /ki reset          ‚Üí Reset rewards (no teleport)
  /testframe         ‚Üí Check IPC connection status
  /name              ‚Üí Toggle nametag overlays
  /agent <id>        ‚Üí Switch to agent ID (changes port)
  /kit create <name> ‚Üí Save current inventory as kit
  /kit load <name>   ‚Üí Load kit
  /kit list          ‚Üí List all kits
  /kit sync          ‚Üí Sync client kits to server

SERVER-SIDE COMMANDS (require mod on server):
  /ki start          ‚Üí Start tracking (sends to port 9998)
  /ki stop           ‚Üí Stop tracking
  /ki reset <p1> <p2> <kit> [shuffle] ‚Üí Reset players with kit
  /team create <name>      ‚Üí Create team
  /team add <team> <player> ‚Üí Add player to team
  /team remove team <name>  ‚Üí Remove team
  /team remove player <name> ‚Üí Remove player from team
  /team list         ‚Üí List all teams
  /team clear        ‚Üí Clear all teams


================================================================================
IMPORTANT NOTES
================================================================================

‚úÖ THE SYSTEM IS COMPLETE - NO CODING REQUIRED!
------------------------------------------------
Everything is already implemented:
  - Neural network model (CNN + Actor-Critic)
  - PPO training algorithm
  - Experience replay buffer
  - GUI for monitoring and configuration
  - Auto-save checkpoints
  - Multi-agent support

All you need to do:
  1. Run: python training_loop.py
  2. Click "Start" in GUI
  3. Launch Minecraft
  4. Watch it train!


‚úÖ REWARD TUNING IS KEY
-----------------------
The default rewards work well, but experiment with:
  - Higher win rewards for more aggressive play
  - Higher time penalties for faster fights
  - Lower damage taken penalties if agent is too passive


‚úÖ TRAINING TAKES TIME
----------------------
- Expect 100-1000 fights before agent shows skill
- Early fights will be random/chaotic
- Gradually agent learns to aim, dodge, combo attacks
- Save checkpoints frequently!


‚úÖ WORKS ON ANY SERVER
----------------------
- Client-side only - no server mod needed
- Hit detection works via client
- Death detection via chat parsing
- Train on vanilla servers, PvP servers, anywhere!


‚úÖ MODEL SAVES AUTOMATICALLY
----------------------------
- Auto-saves after every fight (on death)
- Manual save: Click "Save Model" button
- Checkpoints saved to: checkpoints/pvp_model_YYYYMMDD_HHMMSS.pth
- Load previous checkpoint by modifying training_loop.py


‚úÖ MONITOR METRICS
------------------
Bottom panel shows real-time training metrics:
  - Fights: Total fights completed
  - Updates: PPO optimization steps
  - Policy Loss: How well agent follows policy
  - Value Loss: How accurate value predictions are
  - Entropy: How much exploration is happening


‚úÖ TROUBLESHOOTING TIPS
-----------------------
- If reward stays at 0: Check agent is hitting/being hit
- If agent doesn't move: Check action sending (2 bytes + JSON)
- If model doesn't learn: Try increasing batch_size or epochs in ppo_trainer.py
- If training is slow: Lower frame resolution in FrameCaptureMixin


Good luck with your training! üöÄ
The agent will surprise you with emergent strategies!
================================================================================
